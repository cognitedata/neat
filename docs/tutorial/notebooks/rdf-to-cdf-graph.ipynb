{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating CDF Graph from RDF Graph\n",
    "\n",
    "\n",
    "* author: Nikola Vasiljevic\n",
    "* date: 2023-05-11\n",
    "\n",
    "\n",
    "In this notebook we will describe how NEAT generates CDF graph from RDF graph, which is depicted in the figure below.\n",
    "\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-graph.jpg\"  width=\"75%\" alt=\"RDF2CDF\">\n",
    "\n",
    "\n",
    "The aforementioned image shows high level flow from RDF graph to CDF graph. As we can see an RDF graph can be decoupled to:\n",
    "- nodes (i.e. instances of specific classes)\n",
    "- edges (i.e., relationships that connect nodes)\n",
    "\n",
    "On the other hand CDF graph based on asset-centric data model (aka, classic CDF), consists of:\n",
    "- assets\n",
    "- asset hierarchy \n",
    "- relationships among assets\n",
    "\n",
    "Accordingly, based on the transformation rules, NEAT converts:\n",
    "- RDF nodes to CDF assets\n",
    "- certain type(s) of RDF edges to CDF asset hierarchy\n",
    "- certain type(s) of RDF edges to CDF relationships between assets \n",
    "\n",
    "\n",
    "Let's import all the necessary libraries, create CDF client and mock RDF graph that we will process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from cognite.client import CogniteClient, ClientConfig\n",
    "from cognite.client.credentials import OAuthClientCredentials\n",
    "from rdflib import URIRef\n",
    "\n",
    "from cognite.neat.core import loader, parser, extractors\n",
    "from cognite.neat.core.data_classes.config import RdfStoreType\n",
    "import logging\n",
    "\n",
    "from cognite.neat.core.mocks.graph import add_triples, generate_triples\n",
    "\n",
    "from cognite.neat.core.utils import datetime_utc_now\n",
    "from dotenv import dotenv_values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience store configuration of a Cognite client in `.env` file, with following structure:\n",
    "\n",
    "```\n",
    "\n",
    "TENANT_ID = ...\n",
    "CLIENT_ID = ...\n",
    "CLIENT_SECRET = ...\n",
    "CDF_CLUSTER = ...\n",
    "COGNITE_PROJECT = ...\n",
    "\n",
    "```\n",
    "\n",
    "which we will load as config dictionary and use to configure the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "\n",
    "SCOPES = [f\"https://{config['CDF_CLUSTER']}.cognitedata.com/.default\"]\n",
    "TOKEN_URL = f\"https://login.microsoftonline.com/{config['TENANT_ID']}/oauth2/v2.0/token\"\n",
    "\n",
    "credentials = OAuthClientCredentials(token_url=TOKEN_URL, \n",
    "                                     client_id=config['CLIENT_ID'], \n",
    "                                     client_secret=config['CLIENT_SECRET'], \n",
    "                                     scopes=SCOPES)\n",
    "\n",
    "client_config = ClientConfig(client_name=\"cognite\",\n",
    "                             base_url=f\"https://{config['CDF_CLUSTER']}.cognitedata.com\",\n",
    "                             project=config['COGNITE_PROJECT'],\n",
    "                             credentials=credentials,\n",
    "                             max_workers=1,\n",
    "                             timeout=5 * 60,)\n",
    "\n",
    "client = CogniteClient(client_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load transformation rules and check which classes are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GeographicalRegion',\n",
       " 'Orphanage',\n",
       " 'RootCIMNode',\n",
       " 'SubGeographicalRegion',\n",
       " 'Substation',\n",
       " 'Terminal'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT = Path().resolve().parent.parent.parent\n",
    "TRANSFORMATION_RULES = ROOT / \"cognite\" / \"neat\" / \"examples\" / \"rules\" / \"Rules-Nordic44-to-TNT.xlsx\"\n",
    "\n",
    "\n",
    "raw_sheets = loader.rules.excel_file_to_table_by_name(TRANSFORMATION_RULES)\n",
    "transformation_rules = parser.parse_transformation_rules(raw_sheets)\n",
    "\n",
    "transformation_rules.get_defined_classes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now configure desired number of instances per each of the above classes. We will store desired number of instances in a dictionary which we will call `class_count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = {\"RootCIMNode\":1, \n",
    "               \"GeographicalRegion\":5, \n",
    "               \"SubGeographicalRegion\":10, \n",
    "               \"Substation\": 20, \n",
    "               \"Terminal\": 60}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate mock graph we will first create an empty graph to which we will store triples that will represent our mock graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = loader.NeatGraphStore(prefixes=transformation_rules.prefixes, \n",
    "                                    namespace=transformation_rules.metadata.namespace)\n",
    "graph_store.init_graph(base_prefix=transformation_rules.metadata.prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create triples and then will added them to the graph.\n",
    "\n",
    "The triples are created by providing our data model and desired number of instances per class in a form of dictionary to method `generate_triples`. Afterwards, we will add those triples to our graph using method `add_tripes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_triples = generate_triples(transformation_rules, class_count)\n",
    "add_triples(graph_store, mock_triples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have RDF graph which is stored in memory, we can check if NEAT really created desired RDF graph by checking number of instances (i.e. nodes) per each class. We do this by executing `SPARQL` query against the RDF graph:\n",
    "\n",
    "```\n",
    "SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\n",
    "```\n",
    "\n",
    "which will count number of instances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://purl.org/cognite/tnt#Terminal               60\n",
      "http://purl.org/cognite/tnt#Substation             20\n",
      "http://purl.org/cognite/tnt#SubGeographicalRegion  10\n",
      "http://purl.org/cognite/tnt#GeographicalRegion     5\n",
      "http://purl.org/cognite/tnt#RootCIMNode            1\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\"\n",
    "results = list(graph_store.graph.query(query))\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r[0]:50} {r[1]}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let assume that our CDF dataset does not contain any asset, asset hierarchy and relationship. Accordingly NEAT will go through all the steps shown in the below image to produce CDF graph.\n",
    "In the first step, we will call: \n",
    "- `rdf2assets` which will produce candidate assets to be uploaded to CDF as well asset hierarchy\n",
    "- `categorize_assets` which will categorize this candidate assets against CDF, splitting them to those that are to be `created`, `updated`, `decommissioned` and `resurrected`\n",
    "- `upload_assets` which will upload categorized assets to CDF\n",
    "\n",
    "\n",
    "In the consecutive step, we will call: \n",
    "- `rdf2relationships` which will produce candidate relationships to be uploaded to CDF\n",
    "- `categorize_relationships` which will categorize this candidate relationships against CDF, splitting them to those that are to be `created`, `decommissioned` and `resurrected`. This method will check both existence and state of existing relationships and assets\n",
    "- `upload_relationships` which will upload categorized relationships to CDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../figs/rdf2cdf-init-run.jpg\"  width=\"50%\" alt=\"RDF2CDF\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run each step and inspect results. When running `rdf2assets` there will be ERROR logged regarding `Orphanage`, a special asset expected by NEAT to be in RDF graph. Since we did not created, but defined it in transformation rules, NEAT is logging this as an error, but also fixing issue by creating this asset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while loading instances of class <http://purl.org/cognite/tnt#Orphanage> into cache. Reason: 'instance'\n",
      "WARNING:root:Orphanage with external id orphanage not found in asset hierarchy!\n",
      "WARNING:root:Adding default orphanage with external id orphanage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orphanage with external id orphanage not found in asset hierarchy!\n"
     ]
    }
   ],
   "source": [
    "candidate_assets = extractors.rdf2assets(graph_store, transformation_rules)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected total number of assets is 96 (which we have in RDF graph) plus additional special asset for Orphanage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of assets extracted: 97\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of assets extracted: {len(candidate_assets)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now categorize and upload assets. This completes our first step, i.e. creation of assets and asset hierarchy, as shown below:\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-init-run-step1.jpg\"  width=\"50%\" alt=\"RDF2CDF\">\n",
    "\n",
    "We are expecting to see that there are only assets under category `create`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has 97 assets\n",
      "Category update          has  0 assets\n",
      "Category resurrect       has  0 assets\n",
      "Category decommission    has  0 assets\n"
     ]
    }
   ],
   "source": [
    "categorized_assets = extractors.categorize_assets(client, \n",
    "                                                  candidate_assets, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors.upload_assets(client, categorized_assets, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now re-run `categorize_assets` no asset will be present under any of categories. This means that we have successfully uploaded assets and created asset hierarchy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has  0 assets\n",
      "Category update          has  0 assets\n",
      "Category resurrect       has  0 assets\n",
      "Category decommission    has  0 assets\n"
     ]
    }
   ],
   "source": [
    "categorized_assets = extractors.categorize_assets(client, \n",
    "                                                  candidate_assets, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create, categorize and upload relationships. This completes our last step as shown below:\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-init-run-step2.png\"  width=\"50%\" alt=\"RDF2CDF\">\n",
    "\n",
    "We are expecting to see that there are only relationships under category `create`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has 135 relationships\n",
      "Category decommission    has  0 relationships\n",
      "Category resurrect       has  0 relationships\n"
     ]
    }
   ],
   "source": [
    "candidate_relationships = extractors.rdf2relationships(graph_store, transformation_rules)\n",
    "\n",
    "\n",
    "categorized_relationships = extractors.categorize_relationships(client, \n",
    "                                                  candidate_relationships, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors.upload_relationships(client, categorized_relationships, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to rerunning categorization of assets, we can rerun categorization of relationships and see that no new relationships are created, decommissioned or updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has  0 relationships\n",
      "Category decommission    has  0 relationships\n",
      "Category resurrect       has  0 relationships\n"
     ]
    }
   ],
   "source": [
    "categorized_relationships = extractors.categorize_relationships(client, \n",
    "                                                  candidate_relationships, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now introduce a change in the RDF graph and see how NEAT will react to given change.\n",
    "Specifically we will:\n",
    "- remove node `Substation-13`\n",
    "- remove relation ship between nodes `Substation-3` and `Terminal-3`\n",
    "\n",
    "This should produce graph with reduced number of nodes and edges, as conceptually depicted below:\n",
    "\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-graph-change.jpg\"  width=\"50%\" alt=\"RDF2CDF\">\n",
    "\n",
    "RDF graph original state and state after removing certain parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nfe05f8ef663b4b60a7d6d0209cb1ea99 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Removes all triples from the graph tied to the Substation-13\n",
    "graph_store.graph.remove((transformation_rules.metadata.namespace[\"Substation-13\"], None, None))\n",
    "\n",
    "\n",
    "# Removes only relationship between SubGeographicalRegion-1 and GeographicalRegion-1\n",
    "graph_store.graph.remove((transformation_rules.metadata.namespace[\"Substation-3\"],\n",
    "                          None, \n",
    "                          transformation_rules.metadata.namespace[\"Terminal-3\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the previous process to see if how assets and relationships will change.\n",
    "We are suppose to see following results when comes to assets:\n",
    "\n",
    "- asset `Substation-13` will be decommissioned, unlike RDF graph we never delete assets from CDF graph\n",
    "- asset `Substation-3` will be updated since its metadata field will no longer have field `Substation.Terminal` since we removed this relationship from RDF graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while loading instances of class <http://purl.org/cognite/tnt#Orphanage> into cache. Reason: 'instance'\n",
      "WARNING:root:Orphanage with external id orphanage not found in asset hierarchy!\n",
      "WARNING:root:Adding default orphanage with external id orphanage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orphanage with external id orphanage not found in asset hierarchy!\n",
      "--------------------------------------------\n",
      "Category create          has  0 assets\n",
      "Category update          has  1 assets\n",
      "Category resurrect       has  0 assets\n",
      "Category decommission    has  1 assets\n",
      "--------------------------------------------\n",
      "Asset to decommission Substation-13\n",
      "Asset to update Substation-3\n"
     ]
    }
   ],
   "source": [
    "candidate_assets = extractors.rdf2assets(graph_store, transformation_rules)\n",
    "\n",
    "categorized_assets = extractors.categorize_assets(client, \n",
    "                                                  candidate_assets, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n",
    "print(44*\"-\")\n",
    "\n",
    "\n",
    "print(f\"Asset to decommission {categorized_assets['decommission'][0].external_id}\")\n",
    "print(f\"Asset to update {categorized_assets['update'][0].external_id}\")\n",
    "\n",
    "extractors.upload_assets(client, categorized_assets, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comes to relationships we are suppose to see 7 relationships being decommissioned of which. \n",
    "One being relationship we have explicitly removed from RDF graph, that being `Substation-3` -> `Terminal-3`, while remaining 6 are result of removing `Substation-13`, which are: \n",
    "- `Substation-13` -> `Terminal-53`\n",
    "- `Substation-13` -> `Terminal-33`\n",
    "- `Substation-13` -> `Terminal-13`\n",
    "- `Terminal-53` -> `Substation-13`\n",
    "- `Terminal-33` -> `Substation-13`\n",
    "- `Terminal-13` -> `Substation-13`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Category create          has  0 relationships\n",
      "Category decommission    has  7 relationships\n",
      "Category resurrect       has  0 relationships\n",
      "--------------------------------------------\n",
      "Relationship to decommission: Substation-3:Terminal-3\n",
      "Relationship to decommission: Substation-13:Terminal-53\n",
      "Relationship to decommission: Substation-13:Terminal-13\n",
      "Relationship to decommission: Substation-13:Terminal-33\n",
      "Relationship to decommission: Terminal-13:Substation-13\n",
      "Relationship to decommission: Terminal-33:Substation-13\n",
      "Relationship to decommission: Terminal-53:Substation-13\n"
     ]
    }
   ],
   "source": [
    "candidate_relationships = extractors.rdf2relationships(graph_store, transformation_rules)\n",
    "\n",
    "\n",
    "categorized_relationships = extractors.categorize_relationships(client, \n",
    "                                                  candidate_relationships, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")\n",
    "print(44*\"-\")\n",
    "    \n",
    "for relationship in categorized_relationships[\"decommission\"]:\n",
    "    print(f\"Relationship to decommission: {relationship._external_id}\")\n",
    "    \n",
    "    \n",
    "extractors.upload_relationships(client, categorized_relationships, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire process of updating CDF graph is shown below. As demonstrated through code, what happens is that at core level NEAT performs set operation between RDF and CDF graphs. Specifically, first it find difference in number of nodes (i.e. assets) between CDF and RDF graphs, which identifies which nodes are to be decommissioned (yellow dots). Afterwards it find difference in relationships (i.e., edges) between these two graphs, thus identifying and decommissioning relationships (curved yellow lines). It is important to remember that NEAT never deletes asset or relationship but decommissioned them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../figs/rdf2cdf-post-init.jpg\"  width=\"100%\" alt=\"RDF2CDF\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same process but now bringing back removed node and relationships, thus \"resurrecting\" decommissioned asset and relationships.\n",
    "This is depicted in the figure below.\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-graph-resurrect.jpg\"  width=\"100%\" alt=\"RDF2CDF\">\n",
    "\n",
    "To do this we will add removed triples that make previously removed node and relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_triples(graph_store, mock_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while loading instances of class <http://purl.org/cognite/tnt#Orphanage> into cache. Reason: 'instance'\n",
      "WARNING:root:Orphanage with external id orphanage not found in asset hierarchy!\n",
      "WARNING:root:Adding default orphanage with external id orphanage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orphanage with external id orphanage not found in asset hierarchy!\n",
      "--------------------------------------------\n",
      "Category create          has  0 assets\n",
      "Category update          has  1 assets\n",
      "Category resurrect       has  1 assets\n",
      "Category decommission    has  0 assets\n",
      "--------------------------------------------\n",
      "Asset to resurrect Substation-13\n",
      "Asset to update Substation-3\n"
     ]
    }
   ],
   "source": [
    "candidate_assets = extractors.rdf2assets(graph_store, transformation_rules)\n",
    "\n",
    "categorized_assets = extractors.categorize_assets(client, \n",
    "                                                  candidate_assets, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n",
    "print(44*\"-\")\n",
    "\n",
    "\n",
    "print(f\"Asset to resurrect {categorized_assets['resurrect'][0].external_id}\")\n",
    "print(f\"Asset to update {categorized_assets['update'][0].external_id}\")\n",
    "\n",
    "extractors.upload_assets(client, categorized_assets, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Category create          has  0 relationships\n",
      "Category decommission    has  0 relationships\n",
      "Category resurrect       has  7 relationships\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "candidate_relationships = extractors.rdf2relationships(graph_store, transformation_rules)\n",
    "\n",
    "\n",
    "categorized_relationships = extractors.categorize_relationships(client, \n",
    "                                                  candidate_relationships, \n",
    "                                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")\n",
    "print(44*\"-\")\n",
    "    \n",
    "for relationship in categorized_relationships[\"decommission\"]:\n",
    "    print(f\"Relationship to decommission: {relationship._external_id}\")\n",
    "    \n",
    "extractors.upload_relationships(client, categorized_relationships, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
