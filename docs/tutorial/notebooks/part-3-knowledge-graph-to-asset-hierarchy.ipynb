{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Knowledge Graph to Asset Hierarchy\n",
    "\n",
    "\n",
    "[![Notebook](https://shields.io/badge/notebook-access-green?logo=jupyter&style=for-the-badge)](https://github.com/cognitedata/neat/blob/main/docs/tutorial/part-3-knowledge-graph-to-asset-hierarchy.ipynb)\n",
    "\n",
    "\n",
    "* author: Nikola Vasiljevic\n",
    "* date: 2023-05-11\n",
    "\n",
    "Up until this notebook you did not need to have [Cognite Data Fusion](https://www.cognite.com/en/product/cognite_data_fusion_industrial_dataops_platform) instance running. However, for this notebook, CDF is prerequisite, as well client configured to interact for CDF. How to configure CDF as well client is beyond this notebook and tutorial. We suggest you to find appropriate information at [Cognite Developer Center](https://developer.cognite.com/dev/#authenticate).\n",
    "\n",
    "In this Part 3 of tutorial we will describe how NEAT generates asset hierarchy (i.e., CDF classic graph) from RDF graph (i.e. knowledge graph) as depicted at the following image:\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-graph.jpg\"  width=\"75%\" alt=\"RDF2CDF\">\n",
    "\n",
    "\n",
    "The aforementioned image shows high level flow from RDF graph to CDF graph. As we can see an RDF graph can be decoupled to:\n",
    "- nodes (i.e. instances of specific classes)\n",
    "- edges (i.e., relationships that connect nodes)\n",
    "\n",
    "On the other hand CDF graph based on asset-centric data model (aka, classic CDF), consists of:\n",
    "- assets\n",
    "- asset hierarchy \n",
    "- relationships among assets\n",
    "\n",
    "Accordingly, based on the transformation rules, NEAT converts:\n",
    "- RDF nodes to CDF assets\n",
    "- certain type(s) of RDF edges to CDF asset hierarchy\n",
    "- certain type(s) of RDF edges to CDF relationships between assets \n",
    "\n",
    "\n",
    "First download transformation rules from this [link](https://github.com/cognitedata/neat/blob/main/cognite/neat/examples/rules/source-to-solution-mapping-rules.xlsx) and place it at an appropriate location. Alternatively, if you have cloned `neat` repository, find file in:\n",
    "\n",
    " `./cognite/neat/exaples/rules/source-to-solution-mapping-rules.xlsx`.\n",
    "\n",
    "\n",
    "These transformation rules are a bit more extensive than in previous parts, containing more detailed data model. Their details we will be covered in Part 4. \n",
    "\n",
    "\n",
    "Also, for convenience store configuration of a Cognite client in `.env` file, with following structure:\n",
    "\n",
    "```\n",
    "\n",
    "TENANT_ID = ...\n",
    "CLIENT_ID = ...\n",
    "CLIENT_SECRET = ...\n",
    "CDF_CLUSTER = ...\n",
    "COGNITE_PROJECT = ...\n",
    "\n",
    "```\n",
    "\n",
    "This file will be loaded as config dictionary and used to configure the Cognite client.\n",
    "\n",
    "\n",
    "\n",
    "Let's import all the necessary libraries, create CDF client and mock RDF graph that we will process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from cognite.client import CogniteClient, ClientConfig\n",
    "from cognite.client.credentials import OAuthClientCredentials\n",
    "from cognite.client.config import ClientConfig, global_config\n",
    "\n",
    "\n",
    "from cognite.neat.workflows.examples import source_to_solution_mapping\n",
    "\n",
    "from cognite.neat.rules import load_rules_from_excel_file\n",
    "from cognite.neat.graph.stores import NeatGraphStore\n",
    "\n",
    "from cognite.neat.graph.loaders import rdf2assets, categorize_assets\n",
    "from cognite.neat.graph.loaders import rdf2relationships, categorize_relationships\n",
    "from cognite.neat.graph.loaders import upload_labels, upload_assets, upload_relationships\n",
    "\n",
    "from cognite.neat.utils.utils import remove_namespace, add_triples\n",
    "from cognite.neat.graph.extractors.mocks.graph import generate_triples\n",
    "\n",
    "from dotenv import dotenv_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Insert path to your .env file in \n",
    "\n",
    "config = dotenv_values() if dotenv_values() else dict(TENANT_ID=\"\", CLIENT_ID=\"\", CLIENT_SECRET=\"\", CDF_CLUSTER=\"\", COGNITE_PROJECT=\"\")\n",
    "\n",
    "SCOPES = [f\"https://{config['CDF_CLUSTER']}.cognitedata.com/.default\"]\n",
    "TOKEN_URL = f\"https://login.microsoftonline.com/{config['TENANT_ID']}/oauth2/v2.0/token\"\n",
    "\n",
    "credentials = OAuthClientCredentials(token_url=TOKEN_URL, \n",
    "                                     client_id=config['CLIENT_ID'], \n",
    "                                     client_secret=config['CLIENT_SECRET'], \n",
    "                                     scopes=SCOPES)\n",
    "\n",
    "client_config = ClientConfig(client_name=\"cognite\",\n",
    "                             base_url=f\"https://{config['CDF_CLUSTER']}.cognitedata.com\",\n",
    "                             project=config['COGNITE_PROJECT'],\n",
    "                             credentials=credentials,\n",
    "                             max_workers=1,\n",
    "                             timeout=5 * 60,)\n",
    "\n",
    "client = CogniteClient(client_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load transformation rules and check which classes are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GeographicalRegion',\n",
       " 'Orphanage',\n",
       " 'RootCIMNode',\n",
       " 'SubGeographicalRegion',\n",
       " 'Substation',\n",
       " 'Terminal'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace `source_to_solution_mapping` with Path to your own transformation rules\n",
    "transformation_rules = load_rules_from_excel_file(source_to_solution_mapping)\n",
    "transformation_rules.get_defined_classes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now configure desired number of instances per each of the above classes. We will store desired number of instances in a dictionary which we will call `class_count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = {\"RootCIMNode\":1, \n",
    "               \"GeographicalRegion\":5, \n",
    "               \"SubGeographicalRegion\":10, \n",
    "               \"Substation\": 20, \n",
    "               \"Terminal\": 60}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate mock graph we will first create an empty graph to which we will store triples that will represent our mock graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = NeatGraphStore(prefixes=transformation_rules.prefixes, \n",
    "                             namespace=transformation_rules.metadata.namespace)\n",
    "graph_store.init_graph(base_prefix=transformation_rules.metadata.prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create triples and then will added them to the graph.\n",
    "\n",
    "The triples are created by providing our data model and desired number of instances per class in a form of dictionary to method `generate_triples`. Afterwards, we will add those triples to our graph using method `add_tripes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_triples = generate_triples(transformation_rules, class_count)\n",
    "add_triples(graph_store, mock_triples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have RDF graph which is stored in memory, we can check if NEAT really created desired RDF graph by checking number of instances (i.e. nodes) per each class. We do this by executing `SPARQL` query against the RDF graph:\n",
    "\n",
    "```\n",
    "SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\n",
    "```\n",
    "\n",
    "which will count number of instances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://purl.org/cognite/tnt#Terminal               60\n",
      "http://purl.org/cognite/tnt#Substation             20\n",
      "http://purl.org/cognite/tnt#SubGeographicalRegion  10\n",
      "http://purl.org/cognite/tnt#GeographicalRegion     5\n",
      "http://purl.org/cognite/tnt#RootCIMNode            1\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\"\n",
    "results = list(graph_store.graph.query(query))\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r[0]:50} {r[1]}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let assume that our CDF dataset does not contain any asset, asset hierarchy and relationship. Accordingly NEAT will go through all the steps shown in the below image to produce CDF graph.\n",
    "In the first step, we will call: \n",
    "- `rdf2assets` which will produce candidate assets to be uploaded to CDF as well asset hierarchy\n",
    "- `categorize_assets` which will categorize this candidate assets against CDF, splitting them to those that are to be `created`, `updated`, `decommissioned` and `resurrected`\n",
    "- `upload_assets` which will upload categorized assets to CDF\n",
    "\n",
    "\n",
    "In the consecutive step, we will call: \n",
    "- `rdf2relationships` which will produce candidate relationships to be uploaded to CDF\n",
    "- `categorize_relationships` which will categorize this candidate relationships against CDF, splitting them to those that are to be `created`, `decommissioned` and `resurrected`. This method will check both existence and state of existing relationships and assets\n",
    "- `upload_relationships` which will upload categorized relationships to CDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../figs/rdf2cdf-init-run.jpg\"  width=\"50%\" alt=\"RDF2CDF\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run each step and inspect results. When running `rdf2assets` there will be ERROR logged regarding `Orphanage`, a special asset expected by NEAT to be in RDF graph. Since we did not created, but defined it in transformation rules, NEAT is logging this as an error, but also fixing issue by creating this asset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while loading instances of class <http://purl.org/cognite/tnt#Orphanage> into cache. Reason: 'instance'\n",
      "WARNING:root:Orphanage with external id orphanage-2626756768281823 not found in asset hierarchy!\n",
      "WARNING:root:Adding default orphanage with external id orphanage-2626756768281823\n"
     ]
    }
   ],
   "source": [
    "candidate_assets = rdf2assets(graph_store, transformation_rules)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected total number of assets is 96 (which we have in RDF graph) plus additional special asset for Orphanage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of assets extracted: 97\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of assets extracted: {len(candidate_assets)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now categorize and upload assets. This completes our first step, i.e. creation of assets and asset hierarchy, as shown below:\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-init-run-step1.jpg\"  width=\"50%\" alt=\"RDF2CDF\">\n",
    "\n",
    "We are expecting to see that there are only assets under category `create`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has 97 assets\n",
      "Category update          has  0 assets\n",
      "Category resurrect       has  0 assets\n",
      "Category decommission    has  0 assets\n"
     ]
    }
   ],
   "source": [
    "categorized_assets = categorize_assets(client, \n",
    "                                       candidate_assets, \n",
    "                                       transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we upload assets, we need to create labels which we use to label asset and relationship types as well their status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "upload_labels(client, transformation_rules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now upload assets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "upload_assets(client, categorized_assets, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly inspect created CDF asset hierarchy:\n",
    "\n",
    "<video src=\"../../videos/tutorial-3-asset-hierarchy.mp4\" controls>\n",
    "</video>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now re-run `categorize_assets` no asset will be present under any of categories. This means that we have successfully uploaded assets and created asset hierarchy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has  0 assets\n",
      "Category update          has  0 assets\n",
      "Category resurrect       has  0 assets\n",
      "Category decommission    has  0 assets\n"
     ]
    }
   ],
   "source": [
    "categorized_assets = categorize_assets(client, \n",
    "                                       candidate_assets, \n",
    "                                       transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create, categorize and upload relationships. This completes our last step as shown below:\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-init-run-step2.png\"  width=\"50%\" alt=\"RDF2CDF\">\n",
    "\n",
    "We are expecting to see that there are only relationships under category `create`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has 135 relationships\n",
      "Category resurrect       has  0 relationships\n",
      "Category decommission    has  0 relationships\n"
     ]
    }
   ],
   "source": [
    "candidate_relationships = rdf2relationships(graph_store, transformation_rules)\n",
    "\n",
    "\n",
    "categorized_relationships = categorize_relationships(client, \n",
    "                                                     candidate_relationships, \n",
    "                                                     transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "upload_relationships(client, categorized_relationships, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect relationships that are created for one of the objects:\n",
    "\n",
    "<video src=\"../../videos/tutorial-3-relationships.mp4\" controls>\n",
    "</video>\n",
    "\n",
    "\n",
    "Similarly to rerunning categorization of assets, we can rerun categorization of relationships and see that no new relationships are created, decommissioned or updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category create          has  0 relationships\n",
      "Category resurrect       has  0 relationships\n",
      "Category decommission    has  0 relationships\n"
     ]
    }
   ],
   "source": [
    "categorized_relationships = categorize_relationships(client, \n",
    "                                                     candidate_relationships, \n",
    "                                                     transformation_rules.metadata.data_set_id)\n",
    "\n",
    "\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now introduce a change in the RDF graph and see how NEAT will react to given change.\n",
    "Specifically we will:\n",
    "- remove node `Substation-13`\n",
    "- remove relation ship between nodes `Substation-3` and `Terminal-3`\n",
    "\n",
    "This should produce graph with reduced number of nodes and edges, as conceptually depicted below:\n",
    "\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-graph-change.jpg\"  width=\"50%\" alt=\"RDF2CDF\">\n",
    "\n",
    "RDF graph original state and state after removing certain parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Ne992e5442a75415f8931ce910923ab65 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Removes all triples from the graph tied to the Substation-13\n",
    "graph_store.graph.remove((transformation_rules.metadata.namespace[\"Substation-13\"], None, None))\n",
    "\n",
    "\n",
    "# Removes only relationship between SubGeographicalRegion-1 and GeographicalRegion-1\n",
    "graph_store.graph.remove((transformation_rules.metadata.namespace[\"Substation-3\"],\n",
    "                          None, \n",
    "                          transformation_rules.metadata.namespace[\"Terminal-3\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the previous process to see if how assets and relationships will change.\n",
    "We are suppose to see following results when comes to assets:\n",
    "\n",
    "- asset `Substation-13` will be decommissioned, unlike RDF graph we never delete assets from CDF graph\n",
    "- asset `Substation-3` will be updated since its metadata field will no longer have field `Substation.Terminal` since we removed this relationship from RDF graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while loading instances of class <http://purl.org/cognite/tnt#Orphanage> into cache. Reason: 'instance'\n",
      "WARNING:root:Orphanage with external id orphanage-2626756768281823 not found in asset hierarchy!\n",
      "WARNING:root:Adding default orphanage with external id orphanage-2626756768281823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Category create          has  0 assets\n",
      "Category update          has  1 assets\n",
      "Category resurrect       has  0 assets\n",
      "Category decommission    has  1 assets\n",
      "--------------------------------------------\n",
      "Asset to decommission Substation-13\n",
      "Asset to update Substation-3\n"
     ]
    }
   ],
   "source": [
    "candidate_assets = rdf2assets(graph_store, transformation_rules)\n",
    "\n",
    "categorized_assets = categorize_assets(client, candidate_assets, transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n",
    "print(44*\"-\")\n",
    "\n",
    "\n",
    "print(f\"Asset to decommission {categorized_assets['decommission'][0].external_id}\")\n",
    "print(f\"Asset to update {categorized_assets['update'][0].external_id}\")\n",
    "\n",
    "upload_assets(client, categorized_assets, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comes to relationships we are suppose to see 7 relationships being decommissioned of which. \n",
    "One being relationship we have explicitly removed from RDF graph, that being `Substation-3` -> `Terminal-3`, while remaining 6 are result of removing `Substation-13`, which are: \n",
    "- `Substation-13` -> `Terminal-53`\n",
    "- `Substation-13` -> `Terminal-33`\n",
    "- `Substation-13` -> `Terminal-13`\n",
    "- `Terminal-53` -> `Substation-13`\n",
    "- `Terminal-33` -> `Substation-13`\n",
    "- `Terminal-13` -> `Substation-13`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Category create          has  0 relationships\n",
      "Category resurrect       has  0 relationships\n",
      "Category decommission    has  7 relationships\n",
      "--------------------------------------------\n",
      "Relationship to decommission: Substation-3:Terminal-3\n",
      "Relationship to decommission: Substation-13:Terminal-13\n",
      "Relationship to decommission: Substation-13:Terminal-33\n",
      "Relationship to decommission: Terminal-13:Substation-13\n",
      "Relationship to decommission: Terminal-53:Substation-13\n",
      "Relationship to decommission: Substation-13:Terminal-53\n",
      "Relationship to decommission: Terminal-33:Substation-13\n"
     ]
    }
   ],
   "source": [
    "candidate_relationships = rdf2relationships(graph_store, transformation_rules)\n",
    "\n",
    "\n",
    "categorized_relationships = categorize_relationships(client, \n",
    "                                                     candidate_relationships, \n",
    "                                                     transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")\n",
    "print(44*\"-\")\n",
    "    \n",
    "for relationship in categorized_relationships[\"decommission\"]:\n",
    "    print(f\"Relationship to decommission: {relationship._external_id}\")\n",
    "    \n",
    "    \n",
    "upload_relationships(client, categorized_relationships, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly we will see that assets and associated relationships have been decommissioned in CDF as shown in video below:\n",
    "\n",
    "<video src=\"../../videos/tutorial-3-decommissioning.mp4\" controls>\n",
    "</video>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire process of updating CDF graph is shown below. As demonstrated through code as well in video above, what happens is that at core level NEAT performs set operation between RDF and CDF graphs. Specifically, first it find difference in number of nodes (i.e. assets) between CDF and RDF graphs, which identifies which nodes are to be decommissioned (yellow dots). Afterwards it find difference in relationships (i.e., edges) between these two graphs, thus identifying and decommissioning relationships (curved yellow lines). It is important to remember that NEAT never deletes asset or relationship but decommissioned them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../figs/rdf2cdf-post-init.jpg\"  width=\"100%\" alt=\"RDF2CDF\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same process but now bringing back removed node and relationships, thus \"resurrecting\" decommissioned asset and relationships.\n",
    "This is depicted in the figure below.\n",
    "\n",
    "<img src=\"../../figs/rdf2cdf-graph-resurrect.jpg\"  width=\"100%\" alt=\"RDF2CDF\">\n",
    "\n",
    "To do this we will add removed triples back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_triples(graph_store, mock_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while loading instances of class <http://purl.org/cognite/tnt#Orphanage> into cache. Reason: 'instance'\n",
      "WARNING:root:Orphanage with external id orphanage-2626756768281823 not found in asset hierarchy!\n",
      "WARNING:root:Adding default orphanage with external id orphanage-2626756768281823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Category create          has  0 assets\n",
      "Category update          has  1 assets\n",
      "Category resurrect       has  1 assets\n",
      "Category decommission    has  0 assets\n",
      "--------------------------------------------\n",
      "Asset to resurrect Substation-13\n",
      "Asset to update Substation-3\n"
     ]
    }
   ],
   "source": [
    "candidate_assets = rdf2assets(graph_store, transformation_rules)\n",
    "\n",
    "categorized_assets = categorize_assets(client, candidate_assets, transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_assets:\n",
    "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n",
    "print(44*\"-\")\n",
    "\n",
    "\n",
    "print(f\"Asset to resurrect {categorized_assets['resurrect'][0].external_id}\")\n",
    "print(f\"Asset to update {categorized_assets['update'][0].external_id}\")\n",
    "\n",
    "upload_assets(client, categorized_assets, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Category create          has  0 relationships\n",
      "Category resurrect       has  7 relationships\n",
      "Category decommission    has  0 relationships\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "candidate_relationships = rdf2relationships(graph_store, transformation_rules)\n",
    "\n",
    "\n",
    "categorized_relationships = categorize_relationships(client, candidate_relationships, transformation_rules.metadata.data_set_id)\n",
    "\n",
    "print(44*\"-\")\n",
    "for cat in categorized_relationships:\n",
    "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")\n",
    "print(44*\"-\")\n",
    "    \n",
    "for relationship in categorized_relationships[\"decommission\"]:\n",
    "    print(f\"Relationship to decommission: {relationship._external_id}\")\n",
    "    \n",
    "\n",
    "upload_relationships(client, categorized_relationships, batch_size=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in CDF we will see that the decommissioned asset and relationships have been resurrected as indicated in the video below:\n",
    "\n",
    "<video src=\"../../videos/tutorial-3-resurrection.mp4\" controls>\n",
    "</video>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbreg": {
   "skip": true,
   "skip_reason": "Requires connection to CDF."
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
