{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Knowledge Graph Generation\n",
        "\n",
        "[![Notebook](https://shields.io/badge/notebook-access-green?logo=jupyter&style=for-the-badge)](https://github.com/cognitedata/neat/blob/main/docs/tutorial/notebooks/part-2-knowledge-graph-generation.ipynb)\n",
        "\n",
        "\n",
        "* author: Nikola Vasiljevic\n",
        "* date: 2023-04-03\n",
        "\n",
        "This notebook represent Part 2 of NEAT Onboarding tutorial. If you have not completed previous part(s) we strongly suggest you do them first before doing this part.\n",
        "\n",
        "Often we do not have knowledge graphs per se. Instead we have scattered and unconnected information which needs to be bring together to form knowledge graph.\n",
        "Also, often is useful to test how knowledge graph based on certain data model will function, for example how queries would perform if we have very large knowledge graphs or very deep knowledge graph (many hops).\n",
        "\n",
        "NEAT can help in both cases, i.e. simplify the above scenarios but requires that you have already defined data model through `Transformation Rules`.\n",
        "\n",
        "For this purpose we have prepared simple transformation rules which you can download [using this link](https://github.com/cognitedata/neat/blob/main/cognite/neat/examples/rules/power-grid-example.xlsx). Same Excel sheet is available directly as part of `examples` module of `neat`, as we demonstrated in Part 1 tutorial.\n",
        "\n",
        "\n",
        "In this notebook, we will also demonstrate both scenarios and base the demonstration on the data model defined in Part 1. \n",
        "Accordingly in this notebook we will cover:\n",
        "\n",
        "1. Generation of knowledge graph using Graph Capturing Sheet generated by NEAT\n",
        "2. Generation of mock knowledge graphs of arbitrary size using mock module\n",
        "\n",
        "\n",
        "Let's import all necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'get_class_property_pairs' from 'cognite.neat.core.rules.analysis' (/Volumes/Secondary/repos/neat/cognite/neat/core/rules/analysis.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcognite\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextractors\u001b[39;00m \u001b[39mimport\u001b[39;00m sheet2triples\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcognite\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexporter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrules2graph_sheet\u001b[39;00m \u001b[39mimport\u001b[39;00m rules2graph_capturing_sheet\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcognite\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39manalysis\u001b[39;00m \u001b[39mimport\u001b[39;00m get_class_property_pairs, get_defined_classes\n\u001b[1;32m     11\u001b[0m \u001b[39m# from cognite.neat.core import loader, parser, extractors\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcognite\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m remove_namespace, add_triples\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_class_property_pairs' from 'cognite.neat.core.rules.analysis' (/Volumes/Secondary/repos/neat/cognite/neat/core/rules/analysis.py)"
          ]
        }
      ],
      "source": [
        "from cognite.neat.core.rules import load_rules_from_excel_file\n",
        "from cognite.neat.core.loader import NeatGraphStore\n",
        "from cognite.neat.core.loader.graph_capturing_sheet import excel_file_to_table_by_name\n",
        "\n",
        "from cognite.neat.examples import power_grid_model, power_grid_graph_sheet\n",
        "from cognite.neat.core.extractors import sheet2triples\n",
        "from cognite.neat.core.rules.exporter.rules2graph_sheet import rules2graph_capturing_sheet\n",
        "\n",
        "from cognite.neat.core.rules.analysis import get_class_property_pairs, get_defined_classes\n",
        "\n",
        "# from cognite.neat.core import loader, parser, extractors\n",
        "from cognite.neat.core.utils.utils import remove_namespace, add_triples\n",
        "from cognite.neat.core.mocks.graph import generate_triples, _rules_to_dict\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we already have defined data model in Part 1, we will load it and use it for the rest of this notebook.\n",
        "\n",
        "Here we setting path to the transformation rules which contain data model definition and parsing data model in corresponding form:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformation_rules = load_rules_from_excel_file(power_grid_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now take a look and see how many defined classes we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'GeographicalRegion', 'SubGeographicalRegion', 'Substation', 'Terminal'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_defined_classes(transformation_rules)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now inspect properties related to one of the classes. Here we can see that class `Substation` contains three properties. First property in the list contains value of type string, this type of property in semantic data modeling is known as data type properties, or in general graph theory this property is a node attribute, where node is equivalent to class instance. The remaining property basically contains link to `SubGeographicalRegion` instance. This type of property in the semantic data modeling is known as object properties, while in general graph theory this property represent an edge that connect nodes of two types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>property_type</th>\n",
              "      <th>value_type</th>\n",
              "      <th>min_count</th>\n",
              "      <th>max_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>name</th>\n",
              "      <td>DatatypeProperty</td>\n",
              "      <td>string</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subGeographicalRegion</th>\n",
              "      <td>ObjectProperty</td>\n",
              "      <td>SubGeographicalRegion</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          property_type             value_type min_count  \\\n",
              "name                   DatatypeProperty                 string         1   \n",
              "subGeographicalRegion    ObjectProperty  SubGeographicalRegion         1   \n",
              "\n",
              "                      max_count  \n",
              "name                          1  \n",
              "subGeographicalRegion         1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_rules_to_dict(transformation_rules)['Substation']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use the above data model to generate what we call `Graph Capturing Sheet` which is tailored Excel sheet containing:\n",
        "- sheets for each of the defined classes\n",
        "- columns corresponding to each property defined in data model\n",
        "\n",
        "This sheet is generate using method `rules2graph_capturing_sheet` which is part of `extractors`. The method contains following arguments:\n",
        "\n",
        "- `transformation_rules` : which is instance of transformation rules that contain definition of data model\n",
        "- `file_path`: path where the graph capturing sheet should be stored\n",
        "- `no_rows`: represent expected maximum number rows each sheet will have, thus corresponding to maximum of instance of any of define classes, by default set to 10000\n",
        "- `auto_identifier_type` : type of auto identifier to be made for each class instance, by default set to `index` meaning `index-based` identifiers where index is row number\n",
        "- `add_drop_down_list`: flag indicating whether to provide drop down selection of identifiers (i.e. links) for object type properties (i.e., edges)\n",
        "\n",
        "We will use default values for arguments, meaning, automatic identifiers based on indexes, 10 000 rows, and drop down menus for object type properties:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [],
      "source": [
        "rules2graph_capturing_sheet(transformation_rules, \"./power-grid-graph-capture.xlsx\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the animated gif below one can see how generated graph capturing sheet looks as well how process of capturing graph is conveyed.\n",
        "\n",
        "\n",
        "<video src=\"../../videos/tutorial-2-graph-capturing-sheet.mp4\" controls>\n",
        "</video>\n",
        "\n",
        "\n",
        "A row in a sheet represent an instance of a class. As one enters values for property in column `B`, the identifier is automatically added.\n",
        "As we define instances, their identifier become in drop down menus for properties which are \"edges\" between \"nodes\". By connecting \"nodes\" we make a knowledge graph.\n",
        "\n",
        "Let's now convert now filled graph capturing sheet into knowledge graph. First, we will create empty graph store object, then load raw sheet, and finally convert the raw sheet to graph using previously defined data model in transformation rules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_store = NeatGraphStore(prefixes=transformation_rules.prefixes, \n",
        "                             namespace=transformation_rules.metadata.namespace)\n",
        "\n",
        "graph_store.init_graph(base_prefix=transformation_rules.metadata.prefix)\n",
        "\n",
        "\n",
        "raw_sheets = excel_file_to_table_by_name(power_grid_graph_sheet)\n",
        "triples = sheet2triples(raw_sheets, transformation_rules)\n",
        "\n",
        "add_triples(graph_store, triples)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check graph content we can execute `SPARQL` to count all the class instances:\n",
        "```\n",
        "SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\n",
        "```\n",
        "\n",
        "and later on when processing results we are purposely removing namespaces from the class names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GeographicalRegion        2\n",
            "SubGeographicalRegion     2\n",
            "Substation                2\n",
            "Terminal                  2\n"
          ]
        }
      ],
      "source": [
        "for res in list(graph_store.graph.query(\"SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\")):\n",
        "    print(f\"{remove_namespace(res[0]):25} {res[1]}\" )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, we have two instances of each class that we captured through graph capturing sheet.\n",
        "This completes first possible scenario of using NEAT to create knowledge graph when one does not exist. \n",
        "\n",
        "In the second scenario we will use data model and generate mock graph. \n",
        "We achieve this by configure desired number of instances per each of the above classes. \n",
        "We will store desired number of instances in a dictionary which we will call `class_count`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_count = {\"GeographicalRegion\":5, \n",
        "               \"SubGeographicalRegion\":10, \n",
        "               \"Substation\": 20, \n",
        "               \"Terminal\": 60}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate mock graph we will re-initialize empty graph store, to which we will store triples that will represent our mock graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_store = NeatGraphStore(prefixes=transformation_rules.prefixes, \n",
        "                             namespace=transformation_rules.metadata.namespace)\n",
        "graph_store.init_graph(base_prefix=transformation_rules.metadata.prefix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create triples and then will added them to the graph.\n",
        "\n",
        "The triples are created by providing our data model and desired number of instances per class in a form of dictionary to method `generate_triples`. \n",
        "\n",
        "Afterwards, we will add those triples to our graph using method `add_tripes`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mock_triples = generate_triples(transformation_rules, class_count)\n",
        "add_triples(graph_store, mock_triples)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After successfully creating and adding mock triples let's now take a look at the graph and see if we have expected number of class instances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminal                  60\n",
            "Substation                20\n",
            "SubGeographicalRegion     10\n",
            "GeographicalRegion        5\n"
          ]
        }
      ],
      "source": [
        "for res in list(graph_store.graph.query(\"SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\")):\n",
        "    print(f\"{remove_namespace(res[0]):25} {res[1]}\" )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "neat-V5HCRAva-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "eb4cf576dcffa0a787a8982645e03a998957c06d661d178298ccc8dde5bf89f7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
