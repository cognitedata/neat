{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source to Solution Graph\n",
    "\n",
    "[![Notebook](https://shields.io/badge/notebook-access-green?logo=jupyter&style=for-the-badge)](https://github.com/cognitedata/neat/blob/docs/tutorial/notebooks/source-to-solution-graph.ipynb)\n",
    "\n",
    "* author: Nikola Vasiljevic\n",
    "* date: 2023-02-12\n",
    "\n",
    "\n",
    "In this notebook we will demonstrate key NEAT functionalities and typical workflow. We will work with both knowledge graph provided as file (RDF/XML format) and as well a knowledge graph loaded in dedicated RDF graph database GraphDB. We will use Nordic44 Equipment Profile knowledge graph, which contains number instances which conform to CIM (Common Information Model) ontology. Nordic44 is open source and it is primarily tailored for research purpose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary libraries\n",
    "\n",
    "We will use handful of NEAT methods, which we load in the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from cognite.neat.core.utils import get_cognite_client_from_env\n",
    "from cognite.neat.core import loader, parser\n",
    "from cognite.neat.core.data_classes.config import RdfStoreType\n",
    "from cognite.neat.core.parser.instances import parse_instances\n",
    "from cognite.neat.core.transformer import domain2app_knowledge_graph\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to CDF\n",
    "\n",
    "We have convenience method `get_cognite_client_from_env` which results in cognite sdk client instance which we will use to fetch and push resources from/to CDF. This method requires the environmental variables below to be set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF_CLUSTER = \"az-power-no-northeurope\"\n",
    "TENANT_ID = \"\"\n",
    "\n",
    "os.environ[\"CDF_TOKEN_URL\"] = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
    "os.environ[\"CDF_CLIENT_ID\"] = \"\"\n",
    "os.environ[\"CDF_CLIENT_SECRET\"] = \"\"\n",
    "os.environ[\"CDF_SCOPES\"] = f\"https://{CDF_CLUSTER}.cognitedata.com/.default\"\n",
    "os.environ[\"CDF_CLIENT_NAME\"] = \"cognite\"\n",
    "os.environ[\"CDF_BASE_URL\"] = f\"https://{CDF_CLUSTER}.cognitedata.com\"\n",
    "os.environ[\"CDF_PROJECT\"] = \"get-power-grid\"\n",
    "\n",
    "client = get_cognite_client_from_env()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Transformation Rules\n",
    "All our references "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the transformation rules\n",
    "\n",
    "ROOT = Path().resolve().parent.parent\n",
    "DATA_FOLDER = ROOT / \"data\"\n",
    "TRANSFORMATION_RULES = DATA_FOLDER / \"Rules-Nordic44-to-TNT.xlsx\"\n",
    "IN_MEMORY_KNOWLEDGE_GRAPH = DATA_FOLDER / \"Knowledge-Graph-Nordic44.xml\"\n",
    "\n",
    "raw_sheets = loader.rules.excel_file_to_table_by_name(TRANSFORMATION_RULES)\n",
    "transformation_rules = parser.parse_transformation_rules(raw_sheets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Knowledge Graph to Graph Database\n",
    "In the first part of this tutorial we will work with an instance of `NeatGraphStore` object connected to GraphDB.\n",
    "\n",
    "Accordingly, we need to make sure that we have a running GraphDB on our computer. Follow these steps to achieve this:\n",
    "\n",
    "1. Install and make sure that docker runs\n",
    "2. In terminal go to the root of NEAT repository and execute: \n",
    "\n",
    "    ```make build-docker```\n",
    "\n",
    "3. Make sure to update `docker-compose.yaml` in `./docker` folder with version of GraphDB which suites your computer architecture. Currently it is set to `arm64` which is suitable for M1 and M2 chips.\n",
    "\n",
    "4. After updating `docker-compose.yaml` execute to start containers:\n",
    "\n",
    "    ```make compose-up```\n",
    "\n",
    "5. Go to `http://localhost:7201` in your browser, this should resolve to GraphDB user interface\n",
    "\n",
    "\n",
    "6. Navigate through the user interface and create two repositories, one named `nordic44` another named `tnt`\n",
    "\n",
    "7. In `nordic44` repository load knowledge graph provided as file `Knowledge-Graph-Nordic44.xml` located in `./data`, set `baseIRI` to `http://purl.org/nordic44#`. The `baseIRI` is needed since Nordic44 instances do not contain it, and all instance references are \"relative\", in other words they are missing `namespace`\n",
    "\n",
    "8. Select `nordic44` and navigate to Class Hierarchy and check if you see CIM classes such as Terminal in this repository. Alternatively go to http://localhost:7201/hierarchy\n",
    "\n",
    "If you are unable to create repositories, or have hard time loading knowledge graph checkout the following screen recording (no sound):\n",
    "\n",
    "https://drive.google.com/file/d/1SnS6XvVmuOKOuIaZcsTyt8My8T56OYfS/view?usp=sharing\n",
    "\n",
    "Let's continue and connect to the repositories we made by instantiating and configuring two `NeatGraphStore` objects. \n",
    "These two instances we will call `source_graph` (linked to `nordic44` repository containing source knowledge graph) and `target_graph` (which we will store transformed knowledge to which we will refer as `tnt`).\n",
    "\n",
    "Here we are instantiating object and binding prefixes which can be found in PREFIXES sheet in the transformation rules Excel file. \n",
    "> Checkout other docs to understand why are we using prefixes instead of full namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_graph = loader.NeatGraphStore(prefixes=transformation_rules.prefixes)\n",
    "source_graph.init_graph(\n",
    "    rdf_store_type = RdfStoreType.GRAPHDB,\n",
    "    rdf_store_query_url = \"http://localhost:7201/repositories/nordic44\",\n",
    "    rdf_store_update_url = \"http://localhost:7201/repositories/nordic44/statements\",\n",
    "    graph_name = \"nordic44\",\n",
    ")\n",
    "\n",
    "\n",
    "target_graph = loader.NeatGraphStore(prefixes=transformation_rules.prefixes)\n",
    "target_graph.init_graph(\n",
    "    rdf_store_type = RdfStoreType.GRAPHDB,\n",
    "    rdf_store_query_url = \"http://localhost:7201/repositories/tnt-solution\",\n",
    "    rdf_store_update_url = \"http://localhost:7201/repositories/tnt-solution/statements\",\n",
    "    graph_name = \"tnt-solution\",\n",
    ")\n",
    "target_graph.graph_db_rest_url = \"http://localhost:7201\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if connection to the repositories are correctly established. \n",
    "We will do this by executing very simple SPARQL queries. \n",
    "\n",
    "First lets query against source graph, and request first 10 RDFS classes.\n",
    "This query should return list of tuples containing URIs (i.e., references, globally unique ids) of RDFS classes in Nordic44 knowledge graph. The result will be a mix of base RDFS classes such as `Class`, `Property`, but also classes specific to `CIM` namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace                                          | class name          \n",
      "--------------------------------------------------------------------------------\n",
      "http://iec.ch/TC57/61970-552/ModelDescription/1    | FullModel           \n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | ACLineSegment       \n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | BaseVoltage         \n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | Line                \n",
      "http://entsoe.eu/CIM/SchemaExtension/3/2           | LineCircuit         \n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | ActivePowerLimit    \n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | OperationalLimitSet \n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | OperationalLimitType\n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | Analog              \n",
      "http://entsoe.eu/CIM/SchemaExtension/3/2           | EnergyCongestionZone\n"
     ]
    }
   ],
   "source": [
    "res = list(source_graph.query(\"SELECT DISTINCT ?class WHERE { ?s a ?class } Limit 10\"))\n",
    "\n",
    "print(f\"{'namespace':50} | {'class name':20}\")\n",
    "print(80*\"-\")\n",
    "for i in res:\n",
    "    namespace = i[0].split('#')[0]\n",
    "    print(f\"{i[0].split('#')[0]:50} | {i[0].split('#')[1]:20}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for target repository. Contrary to the nordic44 respository, we should only get handful of standard/base classes from RDFS namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace                                          | class name          \n",
      "--------------------------------------------------------------------------------\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns         | Property            \n",
      "http://www.w3.org/2002/07/owl                      | TransitiveProperty  \n",
      "http://www.w3.org/2002/07/owl                      | SymmetricProperty   \n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns         | List                \n",
      "http://www.w3.org/2000/01/rdf-schema               | Class               \n",
      "http://www.w3.org/2000/01/rdf-schema               | Datatype            \n",
      "http://www.w3.org/2000/01/rdf-schema               | ContainerMembershipProperty\n"
     ]
    }
   ],
   "source": [
    "res = list(target_graph.query(\"SELECT DISTINCT ?class WHERE { ?s a ?class } Limit 10\"))\n",
    "\n",
    "print(f\"{'namespace':50} | {'class name':20}\")\n",
    "print(80*\"-\")\n",
    "for i in res:\n",
    "    namespace = i[0].split('#')[0]\n",
    "    print(f\"{i[0].split('#')[0]:50} | {i[0].split('#')[1]:20}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start transforming Nordic44 knowledge graph (aka source knowledge graph) into TNT knowledge graph (i.e., target knowledge graph).\n",
    "To be sure that we are starting from scratch, we will first delete all triples from the target repository using `.drop()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_graph.drop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual knowledge graph transformation is achieved using method `domain2app_knowledge_graph` which will execute transformation rules one by one.\n",
    "To automatically commit new triples we wrap this method in `NeatGraphStore.set_graph()`. \n",
    "As you can see we are passing couple of arguments to this method, which are:\n",
    "- source knowledge graph\n",
    "- transformation rules\n",
    "- target knowledge graph (this to make sure triples are committed to the graph database as they are being created)\n",
    "- extra triples to be injected to the target knowledge graph (see INSTANCES sheet in the transformation rules Excel file)\n",
    "- instance of Cognite Client (to be able to fetch data from CDF RAW in case of `rawlookup` rules)\n",
    "- CDF RAW database name (to be able to fetch data from CDF RAW in case of `rawlookup` rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_graph.set_graph(\n",
    "            domain2app_knowledge_graph(\n",
    "                source_graph.get_graph(),\n",
    "                transformation_rules,\n",
    "                target_graph.get_graph(),\n",
    "                extra_triples = parse_instances(raw_sheets)\n",
    "                \n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the target repository and see if we have added any new classes or instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace                                          | class name          \n",
      "--------------------------------------------------------------------------------\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns         | Property            \n",
      "http://www.w3.org/2002/07/owl                      | TransitiveProperty  \n",
      "http://www.w3.org/2002/07/owl                      | SymmetricProperty   \n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns         | List                \n",
      "http://www.w3.org/2000/01/rdf-schema               | Class               \n",
      "http://www.w3.org/2000/01/rdf-schema               | Datatype            \n",
      "http://www.w3.org/2000/01/rdf-schema               | ContainerMembershipProperty\n",
      "http://iec.ch/TC57/2013/CIM-schema-cim16           | GeographicalRegion  \n",
      "http://purl.org/cognite/tnt                        | GeographicalRegion  \n",
      "http://purl.org/cognite/tnt                        | SubGeographicalRegion\n"
     ]
    }
   ],
   "source": [
    "res = list(target_graph.query(\"SELECT DISTINCT ?class WHERE { ?s a ?class } Limit 10\"))\n",
    "\n",
    "print(f\"{'namespace':{50}} | {'class name':{20}}\")\n",
    "print(80*\"-\")\n",
    "for i in res:\n",
    "    namespace = i[0].split('#')[0]\n",
    "    print(f\"{i[0].split('#')[0]:{50}} | {i[0].split('#')[1]:{20}}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might noticed that `SubGeographicalRegion` class appears twice in the list. This is because we have used \"*\" wildcard in the transformation rules, which results in copying of all statements (subject, predicate, object) for instances of `http://iec.ch/TC57/2013/CIM-schema-cim16#SubGeographicalRegion` from the source to the target knowledge graph. \n",
    "\n",
    "Among these stamentes there are statements defining that an entity is an instance of class `http://iec.ch/TC57/2013/CIM-schema-cim16#SubGeographicalRegion`. The issue is that while we are doing this, we also stating that these instances in the target knowledge graph are instances of the new class `tnt:SubGeographicalRegion` which we have defined in the transformation rules, hence the \"duplication\". \n",
    "\n",
    "Another issue with this rule type is that you will probably have even more duplication since you will be specifically defining mapping of single properties from source to target knowledge graph (check \"Rule Types in NEAT\"). So, in short words, do not be lazy and explicitly define transformation rules for all classes and their properties you want to transform and avoid using wildcard \"*\" in the transformation rules.\n",
    "\n",
    "\n",
    "But, let's continue and see how corresponding assets will look like in CDF. Let's create them using method `rdf2asset_dictionary`. To this method we are passing following arguments:\n",
    "- target knowledge graph\n",
    "- transformation rules, which contain mapping of RDF classes and properties to CDF Assets and their properties\n",
    "- prefix to external id (useful if multiple people are working with the same CDF project to avoid conflicts in external ids)\n",
    "- external id of Orphanage root asset (this is used in case of RDF instances which are expected to have parent asset, but do not have it defined in the source knowledge graph, so we will assign them to this root asset)\n",
    "\n",
    "\n",
    " and later on categorize them to those that will be:\n",
    "- freshly created\n",
    "- updated\n",
    "- decommissioned (setting their end date, and labeling them as historic)\n",
    "- resurrected (stating date when they are reactivated and removing historic label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_dictionary =  parser.rdf2asset_dictionary(target_graph,\n",
    "                                                transformation_rules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the source knowledge graph there are three \"problematic\" instances, which ended in the target knowledge graph:\n",
    "- An instance of SubGeographicalRegion which is missing relationship to its parent asset\n",
    "- An instance of Terminal that is missing property that maps to CDF Asset name\n",
    "- An instance of Terminal that has alias property that maps to CDF Asset name\n",
    "\n",
    "NEAT manages these instances such that:\n",
    "- An instance of SubGeographicalRegion which is missing relationship to its parent asset will be assigned to Orphanage root asset\n",
    "- An instance of Terminal that is missing property that maps to CDF Asset name will use its identifier with removed namespace as CDF Asset name\n",
    "- An instance of Terminal that has alias property that maps to CDF Asset name will use its alias property as CDF Asset name\n",
    "\n",
    "Let's confirm this by checking the corresponding assets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External ID                          | Name                           | Parent External ID                   | Asset Type          \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "lazarevac                            | LA                             | orphanage                            | GeographicalRegion  \n",
      "f17696b3-9aeb-11e5-91da-b8763fd99c5f | FI1 SGR                        | orphanage                            | SubGeographicalRegion\n",
      "2dd901a4-bdfb-11e5-94fa-c8f73332c8f4 | Alias Name                     | f1769682-9aeb-11e5-91da-b8763fd99c5f | Terminal            \n",
      "terminal-without-name-property       | terminal-without-name-property | f1769688-9aeb-11e5-91da-b8763fd99c5f | Terminal            \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'External ID':36} | {'Name':30} | {'Parent External ID':36} | {'Asset Type':20}\")\n",
    "print(132*\"-\")\n",
    "\n",
    "for id, asset in asset_dictionary.items():\n",
    "    if asset[\"parent_external_id\"] == \"orphanage\" or asset[\"name\"] == \"terminal-without-name-property\" or asset[\"name\"] == \"Alias Name\":\n",
    "        \n",
    "        print(f\"{asset['external_id']:36} | {asset['name']:30} | {asset['parent_external_id']:36} | {asset['metadata']['type']:20}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now categorize assets and see how many of them will be:\n",
    "- freshly created\n",
    "- updated\n",
    "- decommissioned (setting their end date, and labeling them as historic)\n",
    "- resurrected (stating date when they are reactivated and removing historic label)\n",
    "\n",
    "We are passing cognite clinet, asset dictionary and dataset id to the method `categorize_assets` which will return a dictionary with categorized assets.\n",
    "You might see following errors:\n",
    "```\n",
    "ERROR:root:Error while getting non-historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n",
    "ERROR:root:Error while getting historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n",
    "```\n",
    "\n",
    "do not be alarmed, this is because we have not yet created any assets in CDF, so there are no assets to be fetched from CDF.\n",
    "\n",
    "If this is the case the returned dictionary should only have assets under category \"create\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while getting non-historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n",
      "ERROR:root:Error while getting historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: 510\n",
      "update: 0\n",
      "decommission: 0\n",
      "resurrect: 0\n"
     ]
    }
   ],
   "source": [
    "assets = parser.categorize_assets(client, \n",
    "                                  asset_dictionary, \n",
    "                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "for category in assets.keys():\n",
    "    print(f\"{category}: {len(assets[category])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can upload categorized assets to CDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.upload_assets(client, assets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try now again to categorize assets we should see that there are no assets to be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while getting historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: 0\n",
      "update: 0\n",
      "decommission: 0\n",
      "resurrect: 0\n"
     ]
    }
   ],
   "source": [
    "assets = parser.categorize_assets(client, \n",
    "                                  asset_dictionary, \n",
    "                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "for category in assets.keys():\n",
    "    print(f\"{category}: {len(assets[category])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now for sake of demonstration of the NEAT capabilities, decommission \"terminal-without-name-property\" asset and see how NEAT will handle this situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while getting historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: 0\n",
      "update: 0\n",
      "decommission: 1\n",
      "resurrect: 0\n",
      "{\n",
      "    \"external_id\": \"terminal-without-name-property\",\n",
      "    \"name\": \"terminal-without-name-property\",\n",
      "    \"parent_external_id\": \"f1769688-9aeb-11e5-91da-b8763fd99c5f\",\n",
      "    \"data_set_id\": 2626756768281823,\n",
      "    \"metadata\": {\n",
      "        \"IdentifiedObject.aliasName\": \"\",\n",
      "        \"IdentifiedObject.mRID\": \"terminal-without-name-property\",\n",
      "        \"IdentifiedObject.name\": \"\",\n",
      "        \"Terminal.Substation\": \"f1769688-9aeb-11e5-91da-b8763fd99c5f\",\n",
      "        \"active\": \"false\",\n",
      "        \"identifier\": \"terminal-without-name-property\",\n",
      "        \"start_time\": \"2023-03-01 09:47:35.710839\",\n",
      "        \"type\": \"Terminal\",\n",
      "        \"update_time\": \"2023-03-01 09:48:15.216802\",\n",
      "        \"end_time\": \"2023-03-01 09:48:15.216819\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "asset_dictionary.pop(\"terminal-without-name-property\")\n",
    "\n",
    "assets = parser.categorize_assets(client, \n",
    "                                  asset_dictionary, \n",
    "                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "for category in assets.keys():\n",
    "    print(f\"{category}: {len(assets[category])}\")\n",
    "\n",
    "print(assets[\"decommission\"][\"terminal-without-name-property\"])\n",
    "parser.upload_assets(client, assets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now for sake of demonstration of the NEAT capabilities, resurrect \"terminal-without-name-property\" asset and see how NEAT will handle this situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: 0\n",
      "update: 0\n",
      "decommission: 0\n",
      "resurrect: 1\n",
      "{\n",
      "    \"external_id\": \"terminal-without-name-property\",\n",
      "    \"name\": \"terminal-without-name-property\",\n",
      "    \"parent_external_id\": \"f1769688-9aeb-11e5-91da-b8763fd99c5f\",\n",
      "    \"data_set_id\": 2626756768281823,\n",
      "    \"metadata\": {\n",
      "        \"IdentifiedObject.aliasName\": \"\",\n",
      "        \"IdentifiedObject.mRID\": \"terminal-without-name-property\",\n",
      "        \"IdentifiedObject.name\": \"\",\n",
      "        \"Terminal.Substation\": \"f1769688-9aeb-11e5-91da-b8763fd99c5f\",\n",
      "        \"active\": \"true\",\n",
      "        \"identifier\": \"terminal-without-name-property\",\n",
      "        \"start_time\": \"2023-03-01 09:47:35.710839\",\n",
      "        \"type\": \"Terminal\",\n",
      "        \"update_time\": \"2023-03-01 09:49:13.118279\",\n",
      "        \"resurrection_time\": \"2023-03-01 09:49:13.118301\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# this will rebuild asset dictionary\n",
    "asset_dictionary =  parser.rdf2asset_dictionary(target_graph,\n",
    "                                                transformation_rules)\n",
    "\n",
    "\n",
    "\n",
    "assets = parser.categorize_assets(client, \n",
    "                                  asset_dictionary, \n",
    "                                  transformation_rules.metadata.data_set_id)\n",
    "\n",
    "for category in assets.keys():\n",
    "    print(f\"{category}: {len(assets[category])}\")\n",
    "\n",
    "print(assets[\"resurrect\"][\"terminal-without-name-property\"])\n",
    "parser.upload_assets(client, assets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build relationships it is prerequisite to have assets created. \n",
    "Relationships are build in similar way as assets so will chain all methods together to create relationships between assets.\n",
    "\n",
    "Again, if you see error such as:\n",
    "```\n",
    "ERROR:root:Error while getting historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n",
    "```\n",
    "\n",
    "Do not be alarmed, this is because at this point we don't have any historic assets in CDF (i.e. asset that has been decommissioned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while getting historic assets from CDF. Reason: 'DataFrame' object has no attribute 'external_id'\n"
     ]
    }
   ],
   "source": [
    "# Categorize relationships, in case the above assets are not present in CDF\n",
    "# there will be no relationships to create, actually they will be removed\n",
    "relationships = parser.upload_relationships(client,\n",
    "                                            parser.categorize_relationships(client,\n",
    "                                                                            parser.rdf2relationship_dict(target_graph,\n",
    "                                                                                                         transformation_rules),\n",
    "                                                                            transformation_rules.metadata.data_set_id,)\n",
    "                                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to inspect created asset hierarchy and relationships in CDF.\n",
    "\n",
    "Visit:\n",
    "\n",
    "https://cog-get-power.fusion.cognite.com/get-power-grid/explore/search/asset/5166671824792444/asset "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neat-NAW4D3iV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b041fa3ad426ccaee6879971bb0838085ce5b789fde017702a23f1fdae821f14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
