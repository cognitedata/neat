{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Source to Solution Graph\n",
        "\n",
        "[![Notebook](https://shields.io/badge/notebook-access-green?logo=jupyter&style=for-the-badge)](https://github.com/cognitedata/neat/blob/docs/tutorial/notebooks/part-4-knowledge-graph-transformation.ipynb)\n",
        "\n",
        "* author: Nikola Vasiljevic\n",
        "* date: 2023-02-12\n",
        "\n",
        "\n",
        "In this Part 4 of tutorial series we will most comprehensive feature of NEAT and that is transformation of source knowledge graph to solution graph and the latter conversion to CDF asset hierarchy. We will work with Nordic44 knowledge graph sourced from the RDF/XML file. We will use Nordic44 Equipment Profile knowledge graph, which contains number instances which conform to CIM (Common Information Model) data model. Nordic44 is open source and it is primarily tailored for research purpose.\n",
        "\n",
        "First download necessary files:\n",
        "- [Transformation rules](https://github.com/cognitedata/neat/blob/main/cognite/neat/examples/rules/source-to-solution-mapping-rules.xlsx)\n",
        "- [Nordic44 knowledge graph](https://github.com/cognitedata/neat/blob/main/cognite/neat/examples/source-graphs/Knowledge-Graph-Nordic44.xml)\n",
        "\n",
        "and placed them at convenient location for loading in this notebook.\n",
        "\n",
        "\n",
        "Alternatively, you can import them from `neat` examples module as, which is the approach we will use in this tutorial\n",
        "\n",
        "\n",
        "Also, for convenience store configuration of a Cognite client in `.env` file, with following structure:\n",
        "\n",
        "```\n",
        "\n",
        "TENANT_ID = ...\n",
        "CLIENT_ID = ...\n",
        "CLIENT_SECRET = ...\n",
        "CDF_CLUSTER = ...\n",
        "COGNITE_PROJECT = ...\n",
        "\n",
        "```\n",
        "\n",
        "This file will be loaded as config dictionary and used to configure the Cognite client.\n",
        "\n",
        "\n",
        "Once you located necessary files, created `.env` file, load necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from cognite.client import CogniteClient, ClientConfig\n",
        "from cognite.client.credentials import OAuthClientCredentials\n",
        "from cognite.client.config import ClientConfig, global_config\n",
        "\n",
        "\n",
        "from cognite.neat.examples import source_to_solution_mapping, nordic44_knowledge_graph\n",
        "\n",
        "from cognite.neat.core.rules import load_rules_from_excel_file\n",
        "from cognite.neat.core.loader import NeatGraphStore\n",
        "\n",
        "from cognite.neat.core.extractors import rdf2assets, categorize_assets\n",
        "from cognite.neat.core.extractors import rdf2relationships, categorize_relationships\n",
        "from cognite.neat.core.extractors import upload_labels, upload_assets, upload_relationships\n",
        "\n",
        "from cognite.neat.core.utils.utils import remove_namespace, add_triples\n",
        "from cognite.neat.core.mocks.graph import generate_triples\n",
        "\n",
        "from cognite.neat.core.transformer import source2solution_graph\n",
        "\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's instantiate CDF client in same why we did in Part 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert path to your .env file in \n",
        "config = dotenv_values(\"insert_path_to_your_env_file\")\n",
        "\n",
        "SCOPES = [f\"https://{config['CDF_CLUSTER']}.cognitedata.com/.default\"]\n",
        "TOKEN_URL = f\"https://login.microsoftonline.com/{config['TENANT_ID']}/oauth2/v2.0/token\"\n",
        "\n",
        "credentials = OAuthClientCredentials(token_url=TOKEN_URL, \n",
        "                                     client_id=config['CLIENT_ID'], \n",
        "                                     client_secret=config['CLIENT_SECRET'], \n",
        "                                     scopes=SCOPES)\n",
        "\n",
        "client_config = ClientConfig(client_name=\"cognite\",\n",
        "                             base_url=f\"https://{config['CDF_CLUSTER']}.cognitedata.com\",\n",
        "                             project=config['COGNITE_PROJECT'],\n",
        "                             credentials=credentials,\n",
        "                             max_workers=1,\n",
        "                             timeout=5 * 60,)\n",
        "\n",
        "client = CogniteClient(client_config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load transformation rules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'GeographicalRegion',\n",
              " 'Orphanage',\n",
              " 'RootCIMNode',\n",
              " 'SubGeographicalRegion',\n",
              " 'Substation',\n",
              " 'Terminal'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Replace `source_to_solution_mapping` with Path to your own transformation rules\n",
        "transformation_rules = load_rules_from_excel_file(source_to_solution_mapping)\n",
        "transformation_rules.get_defined_classes()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike Part 3, in this tutorial we need to create two instances of graph stores, one to hold triples of source graph, and second to hold triples of solution graph.\n",
        "As we are loading existing graph to source we will need to specify its namespace, which we have conveniently stored in `Prefixes` sheet under `nordic44`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_store = NeatGraphStore(prefixes=transformation_rules.prefixes, \n",
        "                              namespace=transformation_rules.prefixes[\"nordic44\"])\n",
        "source_store.init_graph(base_prefix=transformation_rules.metadata.prefix)\n",
        "\n",
        "\n",
        "solution_store = NeatGraphStore(prefixes=transformation_rules.prefixes, \n",
        "                                namespace=transformation_rules.metadata.namespace)\n",
        "solution_store.init_graph(base_prefix=transformation_rules.metadata.prefix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's import Nordic44 triples to `source_store`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_store.import_from_file(nordic44_knowledge_graph)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This query should return list of tuples containing URIs (i.e., references, globally unique ids) of RDFS classes in Nordic44 knowledge graph. The result will be a mix of base RDFS classes such as `Class`, `Property`, but also classes specific to `CIM` namespace:\n",
        "\n",
        "Let's list top 20 classes and number of their instances like we did in previous tutorials. As will see the number of substations is 44, reason why Nordic44 has 44 in its name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "namespace            | class name          \n",
            "----------------------------------------\n",
            "CurrentLimit         | 530                 \n",
            "Terminal             | 452                 \n",
            "OperationalLimitSet  | 238                 \n",
            "OperatingShare       | 207                 \n",
            "VoltageLimit         | 184                 \n",
            "AnalogValue          | 133                 \n",
            "ConnectivityNode     | 89                  \n",
            "GeneratingUnit       | 80                  \n",
            "SynchronousMachine   | 80                  \n",
            "ACLineSegment        | 68                  \n",
            "Line                 | 68                  \n",
            "BusbarSection        | 46                  \n",
            "VoltageLevel         | 45                  \n",
            "Substation           | 44                  \n",
            "ConformLoad          | 35                  \n",
            "ConformLoadGroup     | 35                  \n",
            "Analog               | 30                  \n",
            "Breaker              | 29                  \n",
            "Disconnector         | 26                  \n",
            "PowerTransformerEnd  | 24                  \n",
            "RegulatingControl    | 18                  \n",
            "Bay                  | 16                  \n"
          ]
        }
      ],
      "source": [
        "print(f\"{'namespace':20} | {'class name':20}\")\n",
        "print(40*\"-\")\n",
        "\n",
        "for i, res in enumerate(list(source_store.graph.query(\"SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\"))):\n",
        "    print(f\"{remove_namespace(res[0]):20} | {res[1]:20}\" )\n",
        "    if i > 20:\n",
        "        break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we try to do the same for `solution_store` we will that it is empty:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "namespace            | class name          \n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(f\"{'namespace':20} | {'class name':20}\")\n",
        "print(40*\"-\")\n",
        "\n",
        "for i, res in enumerate(list(solution_store.graph.query(\"SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\"))):\n",
        "    print(f\"{remove_namespace(res[0]):20} | {res[1]:20}\" )\n",
        "    if i > 20:\n",
        "        break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you closely inspect transformation rules, we are not interested in all classes that are represented by Nordic44 knowledge graph, but only selected few.\n",
        "Furthermore, as described in [rule types](../../rule-types.md), often source graph are deep and one is required to perform multiple hops to acquire a specific information.\n",
        "Therefore, it is convenient to \"short path\", thus transform graph to be more performant, basically flattening graph structure to help us achieve simpler queries.\n",
        "\n",
        "You can see that this is exactly what we are doing in case of links between substations and terminals. We are greatly reducing the traversal path.\n",
        "\n",
        "To perform transformations described in the transformation rules, The actual knowledge graph transformation is achieved using method `domain2app_knowledge_graph` which will execute transformation rules one by one.\n",
        "To automatically commit new triples we wrap this method in `NeatGraphStore.set_graph()`. \n",
        "As you can see we are passing couple of arguments to this method, which are:\n",
        "- source knowledge graph\n",
        "- transformation rules\n",
        "- target knowledge graph (this to make sure triples are committed to the graph database as they are being created)\n",
        "- extra triples to be injected to the target knowledge graph (see INSTANCES sheet in the transformation rules Excel file)\n",
        "- instance of Cognite Client (to be able to fetch data from CDF RAW in case of `rawlookup` rules)\n",
        "- CDF RAW database name (to be able to fetch data from CDF RAW in case of `rawlookup` rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "solution_store.set_graph(source2solution_graph(source_store.get_graph(),\n",
        "                                               transformation_rules,\n",
        "                                               solution_store.get_graph(),\n",
        "                                               extra_triples = transformation_rules.get_instances_as_triples())\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now inspect graph `solution_store` and see breakdown of number of instances per class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "namespace                      | class name          \n",
            "----------------------------------------\n",
            "Terminal                       | 452                 \n",
            "Substation                     | 44                  \n",
            "SubGeographicalRegion          | 10                  \n",
            "GeographicalRegion             | 2                   \n",
            "RootCIMNode                    | 1                   \n"
          ]
        }
      ],
      "source": [
        "print(f\"{'namespace':30} | {'class name':20}\")\n",
        "print(40*\"-\")\n",
        "\n",
        "for res in list(solution_store.graph.query(\"SELECT ?class (count(?s) as ?instances ) WHERE { ?s a ?class . } group by ?class order by DESC(?instances)\")):\n",
        "    print(f\"{remove_namespace(res[0]):30} | {res[1]:20}\" )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As one can notice, with the transformation rules we have cherry picked classes existing or created new properties to suite out need.\n",
        "\n",
        "Let's continue and see how corresponding assets will look like in CDF. We will use same methods as one in Part 3 notebook.\n",
        "\n",
        "<!-- Let's create them using method `rdf2asset`. To this method we are passing following arguments:\n",
        "- target knowledge graph\n",
        "- transformation rules, which contain mapping of RDF classes and properties to CDF Assets and their properties\n",
        "- prefix to external id (useful if multiple people are working with the same CDF project to avoid conflicts in external ids)\n",
        "- external id of Orphanage root asset (this is used in case of RDF instances which are expected to have parent asset, but do not have it defined in the source knowledge graph, so we will assign them to this root asset)\n",
        "\n",
        "\n",
        " and later on categorize them to those that will be:\n",
        "- freshly created\n",
        "- updated\n",
        "- decommissioned (setting their end date, and labeling them as historic)\n",
        "- resurrected (stating date when they are reactivated and removing historic label) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Error while loading instances of class <http://purl.org/cognite/tnt#Orphanage> into cache. Reason: 'instance'\n",
            "WARNING:root:Orphanage with external id orphanage-2626756768281823 not found in asset hierarchy!\n",
            "WARNING:root:Adding default orphanage with external id orphanage-2626756768281823\n"
          ]
        }
      ],
      "source": [
        "candidate_assets = rdf2assets(solution_store, transformation_rules)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have corrupted Nordic44 knowledge graph to show you how `NEAT` handles missing properties or isolated nodes.\n",
        "\n",
        "In the source knowledge graph there are three \"problematic\" instances, which ended in the target knowledge graph:\n",
        "- An instance of GeographicalRegion which is missing relationship to its parent asset, specifically `RootCIMNode`\n",
        "- An instance of SubGeographicalRegion which is missing relationship to a `GeographicalRegion`, i.e. its parent asset\n",
        "- An instance of Terminal that is missing property that maps to CDF Asset name\n",
        "- An instance of Terminal that has alias property that maps to CDF Asset name\n",
        "\n",
        "NEAT manages these instances such that:\n",
        "- An instance of GeographicalRegion and SubGeographicalRegion which is missing relationship to its parent asset will be assigned to Orphanage root asset\n",
        "- An instance of Terminal that is missing property that maps to CDF Asset name will use its identifier with removed namespace as CDF Asset name\n",
        "- An instance of Terminal that has alias property that maps to CDF Asset name will use its alias property as CDF Asset name\n",
        "\n",
        "Let's confirm this by checking the corresponding assets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "External ID                          | Name                           | Parent External ID                   | Asset Type          \n",
            "------------------------------------------------------------------------------------------------------------------------------------\n",
            "lazarevac                            | LA                             | orphanage-2626756768281823           | GeographicalRegion  \n",
            "f17696b3-9aeb-11e5-91da-b8763fd99c5f | FI1 SGR                        | orphanage-2626756768281823           | SubGeographicalRegion\n",
            "2dd901a4-bdfb-11e5-94fa-c8f73332c8f4 | Alias Name                     | f1769682-9aeb-11e5-91da-b8763fd99c5f | Terminal            \n",
            "terminal-without-name-property       | terminal-without-name-property | f1769688-9aeb-11e5-91da-b8763fd99c5f | Terminal            \n"
          ]
        }
      ],
      "source": [
        "print(f\"{'External ID':36} | {'Name':30} | {'Parent External ID':36} | {'Asset Type':20}\")\n",
        "print(132*\"-\")\n",
        "\n",
        "for id, asset in candidate_assets.items():\n",
        "    if asset[\"parent_external_id\"] == \"orphanage-2626756768281823\" or asset[\"name\"] == \"terminal-without-name-property\" or asset[\"name\"] == \"Alias Name\":\n",
        "        \n",
        "        print(f\"{asset['external_id']:36} | {asset['name']:30} | {asset['parent_external_id']:36} | {asset['metadata']['type']:20}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now categorize assets and see how many of them will be:\n",
        "- created\n",
        "- updated\n",
        "- decommissioned (setting their end date, and labeling them as historic)\n",
        "- resurrected (stating date when they are reactivated and removing historic label)\n",
        "\n",
        "We are passing cognite clinet, asset dictionary and dataset id to the method `categorize_assets` which will return a dictionary with categorized assets.\n",
        "If this is the case the returned dictionary should only have assets under category \"create\", since there are no assets in CDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category create          has 510 assets\n",
            "Category update          has  0 assets\n",
            "Category resurrect       has  0 assets\n",
            "Category decommission    has  0 assets\n"
          ]
        }
      ],
      "source": [
        "categorized_assets = categorize_assets(client, \n",
        "                                       candidate_assets, \n",
        "                                       transformation_rules.metadata.data_set_id)\n",
        "\n",
        "\n",
        "for cat in categorized_assets:\n",
        "    print(f\"Category {cat:15} has {len(categorized_assets[cat]):2} assets\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we upload assets, we need to create labels which we use to label asset and relationship types as well their status:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "upload_labels(client, transformation_rules)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we can upload categorized assets to CDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "upload_assets(client, categorized_assets, batch_size=1000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now for sake of completeness check if two orphaned assets are added under Orphanage and that two terminals with missing and alias names have their names properly fixed:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's repeat process for relationships:\n",
        "\n",
        "\n",
        "<video src=\"../../videos/tutorial-4-asset-hierarchy.mp4\" controls>\n",
        "</video>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category create          has 915 relationships\n",
            "Category resurrect       has  0 relationships\n",
            "Category decommission    has  0 relationships\n"
          ]
        }
      ],
      "source": [
        "candidate_relationships = rdf2relationships(solution_store, transformation_rules)\n",
        "\n",
        "\n",
        "categorized_relationships = categorize_relationships(client, \n",
        "                                                     candidate_relationships, \n",
        "                                                     transformation_rules.metadata.data_set_id)\n",
        "\n",
        "\n",
        "for cat in categorized_relationships:\n",
        "    print(f\"Category {cat:15} has {len(categorized_relationships[cat]):2} relationships\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "upload_relationships(client, categorized_relationships, batch_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "neat-NAW4D3iV-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "nbreg": {
      "skip": true,
      "skip_reason": "Requires connection to CDF."
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b041fa3ad426ccaee6879971bb0838085ce5b789fde017702a23f1fdae821f14"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
